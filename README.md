
# ğŸ“š Modular RAG PDF Chatbot

A modular **Retrieval-Augmented Generation (RAG)** chatbot that lets you upload PDFs and interact with an AI assistant. The assistant answers queries based on your documents using a **FastAPI backend**, **Streamlit frontend**, **ChromaDB vector store**, and **Groqâ€™s LLaMA3 model**.

---

## ğŸ— Project Structure

```
PDFragbot/
â”œâ”€â”€ client/         # Streamlit Frontend
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ chatUI.py
â”‚   â”‚   â”œâ”€â”€ history_download.py
â”‚   â”‚   â””â”€â”€ upload.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ api.py
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ server/         # FastAPI Backend
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ load_vectorstore.py
â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â”œâ”€â”€ pdf_handler.py
â”‚   â”‚   â””â”€â”€ query_handlers.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ uploaded_pdfs/   # auto-created after run
â”‚   â””â”€â”€ chroma_store/    # auto-created after run
â””â”€â”€ README.md
```

---

## âœ¨ Features

* ğŸ“‚ Upload and parse PDFs
* ğŸ§© Chunk documents + generate embeddings (HuggingFace)
* ğŸ—„ Store embeddings in **ChromaDB**
* ğŸ¤– Query docs with **Groq LLaMA3**
* âš¡ Microservice design â†’ **FastAPI backend + Streamlit frontend**

---

## ğŸ” How RAG Works

**Retrieval-Augmented Generation (RAG)** = LLM + External Knowledge.

1. Documents are chunked and embedded â†’ stored in a vector database (ChromaDB).
2. At query time, relevant chunks are retrieved.
3. LLaMA3 combines the query + retrieved context â†’ generates accurate, context-aware answers.

---

## ğŸš€ Getting Started

### 1. Clone Repository

```bash
git clone https://github.com/Ashish-prajapat08/PDFragbot.git
cd ragbot2.0
```

### 2. Setup Backend (FastAPI)

```bash
cd server
python -m venv venv
source venv/bin/activate    # Windows: venv\Scripts\activate
pip install -r requirements.txt

# Add your Groq API Key
echo "GROQ_API_KEY=your_api_key_here" > .env

# Run FastAPI server
uvicorn main:app --reload
```

### 3. Setup Frontend (Streamlit)

```bash
cd ../client
pip install -r requirements.txt
streamlit run app.py
```

---

## ğŸŒ API Endpoints

* `POST /upload_pdfs/` â†’ Upload PDFs & build vectorstore
* `POST /ask/` â†’ Query PDFs & get AI response

ğŸ‘‰ Test via **Postman** or directly from the Streamlit client.

---

## ğŸ›  Roadmap

* [ ] Add authentication
* [ ] Dockerize frontend & backend
* [ ] Support more file formats (DOCX, TXT, etc.)

---

âš¡ With this modular design, you can easily extend the chatbot with new features, models, or storage backends.

